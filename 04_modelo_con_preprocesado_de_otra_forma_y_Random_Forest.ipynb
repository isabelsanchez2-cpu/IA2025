{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNAhBA376oW6u4AshzrwNAR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isabelsanchez2-cpu/IA2025/blob/main/04_modelo_con_preprocesado_de_otra_forma_y_Random_Forest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "z3yH1Kfpe4W3",
        "outputId": "2cc7a3b1-dfcd-4d9d-abf4-f7f93ae2d3bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Librerías cargadas\n",
            "Train: (692500, 21)\n",
            "Test: (296786, 20)\n",
            "\n",
            "Distribución de clases:\n",
            "RENDIMIENTO_GLOBAL\n",
            "alto          175619\n",
            "bajo          172987\n",
            "medio-alto    171619\n",
            "medio-bajo    172275\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Numéricas: 5\n",
            "Categóricas: 14\n",
            "\n",
            "--- Limpieza ---\n",
            "\n",
            "--- Imputación ---\n",
            "✓ Imputación completada\n",
            "\n",
            "--- Codificación ---\n",
            "✓ Codificadas 14 columnas\n",
            "\n",
            "✓ Datos listos\n",
            "  NaN en train: 0\n",
            "  NaN en test: 0\n",
            "\n",
            "--- Preparando target ---\n",
            "✓ Clases originales: ['alto' 'bajo' 'medio-alto' 'medio-bajo']\n",
            "✓ Clases codificadas: [0 1 2 3]\n",
            "\n",
            "--- Entrenando XGBoost ---\n",
            "XGBoost es un gradient boosting que construye árboles secuencialmente\n",
            "\n",
            "Entrenando modelo...\n",
            "✓ XGBoost entrenado\n",
            "\n",
            "✓ Accuracy en train: 0.4902\n",
            "\n",
            "--- Validación Cruzada ---\n",
            "CV Scores: [0.4323 0.4326 0.4331]\n",
            "✓ CV Accuracy: 0.4327 (+/- 0.0004)\n",
            "\n",
            "--- Feature Importance (Top 15) ---\n",
            "                    feature  importance\n",
            "E_VALORMATRICULAUNIVERSIDAD    0.177893\n",
            "          F_ESTRATOVIVIENDA    0.118196\n",
            "      E_PAGOMATRICULAPROPIO    0.099557\n",
            "          F_TIENEINTERNET.1    0.070717\n",
            "          F_TIENECOMPUTADOR    0.061643\n",
            "           E_PRGM_ACADEMICO    0.059967\n",
            "            F_TIENEINTERNET    0.052852\n",
            "           F_EDUCACIONMADRE    0.050846\n",
            "        E_PRGM_DEPARTAMENTO    0.049198\n",
            "                INDICADOR_1    0.044979\n",
            "       E_HORASSEMANATRABAJA    0.036295\n",
            "          PERIODO_ACADEMICO    0.035747\n",
            "           F_EDUCACIONPADRE    0.032270\n",
            "           F_TIENEAUTOMOVIL    0.023094\n",
            "            F_TIENELAVADORA    0.020987\n",
            "\n",
            "--- Predicciones ---\n",
            "✓ Predicciones: 296786\n",
            "\n",
            "Distribución de predicciones:\n",
            "alto          86595\n",
            "bajo          90735\n",
            "medio-alto    59229\n",
            "medio-bajo    60227\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Porcentaje:\n",
            "alto          29.18\n",
            "bajo          30.57\n",
            "medio-alto    19.96\n",
            "medio-bajo    20.29\n",
            "Name: count, dtype: float64\n",
            "\n",
            "✓ Submission shape: (296786, 2)\n",
            "✓ IDs únicos: 296786\n",
            "✓ Clases predichas: ['alto', 'bajo', 'medio-alto', 'medio-bajo']\n",
            "\n",
            "Primeras filas:\n",
            "       ID RENDIMIENTO_GLOBAL\n",
            "0  550236               alto\n",
            "1   98545         medio-alto\n",
            "2  499179               alto\n",
            "3  782980               bajo\n",
            "4  785185               bajo\n",
            "5   58495               bajo\n",
            "6  705444               alto\n",
            "7  557548               alto\n",
            "8  519909               bajo\n",
            "9  832058               alto\n",
            "\n",
            "============================================================\n",
            "✓✓✓ Archivo submission_xgboost.csv generado ✓✓✓\n",
            "============================================================\n",
            "\n",
            "Este es un modelo alternativo usando XGBoost\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n## Resumen Modelo 04\\n\\n### XGBoost (Gradient Boosting)\\n- **Tipo**: Gradient Boosting Machine\\n- **Árboles**: 200 (secuenciales, no paralelos)\\n- **Learning rate**: 0.1\\n- **Max depth**: 8\\n\\n### Ventajas de XGBoost\\n- ✅ Muy efectivo en competencias Kaggle\\n- ✅ Maneja bien features categóricas codificadas\\n- ✅ Regularización incorporada\\n- ✅ Feature importance integrado\\n\\n### Diferencias con otros modelos\\n\\n| Característica | RF (99) | SVM (03) | XGBoost (04) |\\n|----------------|---------|----------|--------------|\\n| Construcción | Paralela | N/A | Secuencial |\\n| Velocidad | Rápido | Lento | Media |\\n| Overfitting | Bajo | Medio | Controlable |\\n| Interpretabilidad | Alta | Baja | Alta |\\n\\n### Conclusión\\nXGBoost es una excelente alternativa que frecuentemente da buenos\\nresultados en problemas de clasificación tabulares.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# 04 - MODELO GRADIENT BOOSTING (XGBoost)\n",
        "# ============================================================================\n",
        "\n",
        "# %% [markdown]\n",
        "\"\"\"\n",
        "# 04 - Modelo con Gradient Boosting (XGBoost)\n",
        "\n",
        "## Estrategia\n",
        "Este notebook utiliza XGBoost, un algoritmo de gradient boosting que suele dar\n",
        "excelentes resultados en competencias de Kaggle.\n",
        "\n",
        "## Diferencias con modelo 99:\n",
        "- Usa XGBoost en lugar de Random Forest\n",
        "- Gradient Boosting construye árboles secuencialmente\n",
        "- Incluye análisis de importancia de features\n",
        "- Parámetros optimizados para clasificación multiclase\n",
        "\"\"\"\n",
        "\n",
        "# %% Importar librerías\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print('✓ Librerías cargadas')\n",
        "\n",
        "# %% Cargar datos\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "\n",
        "print(f'Train: {train.shape}')\n",
        "print(f'Test: {test.shape}')\n",
        "\n",
        "# %% Distribución de clases\n",
        "print('\\nDistribución de clases:')\n",
        "print(train['RENDIMIENTO_GLOBAL'].value_counts().sort_index())\n",
        "\n",
        "# %% Separar componentes\n",
        "train_ids = train['ID'].copy()\n",
        "test_ids = test['ID'].copy()\n",
        "y_train = train['RENDIMIENTO_GLOBAL'].copy()\n",
        "\n",
        "X_train = train.drop(['ID', 'RENDIMIENTO_GLOBAL'], axis=1)\n",
        "X_test = test.drop(['ID'], axis=1)\n",
        "\n",
        "# %% Identificar columnas\n",
        "numeric_cols = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "categorical_cols = X_train.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "print(f'\\nNuméricas: {len(numeric_cols)}')\n",
        "print(f'Categóricas: {len(categorical_cols)}')\n",
        "\n",
        "# %% Limpieza\n",
        "print('\\n--- Limpieza ---')\n",
        "for col in categorical_cols:\n",
        "    X_train[col] = X_train[col].replace(['', ' ', 'nan'], np.nan)\n",
        "    X_test[col] = X_test[col].replace(['', ' ', 'nan'], np.nan)\n",
        "\n",
        "# %% Imputación\n",
        "print('\\n--- Imputación ---')\n",
        "if len(numeric_cols) > 0:\n",
        "    num_imp = SimpleImputer(strategy='median')\n",
        "    X_train[numeric_cols] = num_imp.fit_transform(X_train[numeric_cols])\n",
        "    X_test[numeric_cols] = num_imp.transform(X_test[numeric_cols])\n",
        "\n",
        "if len(categorical_cols) > 0:\n",
        "    cat_imp = SimpleImputer(strategy='most_frequent')\n",
        "    X_train[categorical_cols] = cat_imp.fit_transform(X_train[categorical_cols])\n",
        "    X_test[categorical_cols] = cat_imp.transform(X_test[categorical_cols])\n",
        "\n",
        "print('✓ Imputación completada')\n",
        "\n",
        "# %% Codificación\n",
        "print('\\n--- Codificación ---')\n",
        "for col in categorical_cols:\n",
        "    le = LabelEncoder()\n",
        "    combined = pd.concat([X_train[col].astype(str), X_test[col].astype(str)])\n",
        "    le.fit(combined)\n",
        "    X_train[col] = le.transform(X_train[col].astype(str))\n",
        "    X_test[col] = le.transform(X_test[col].astype(str))\n",
        "\n",
        "print(f'✓ Codificadas {len(categorical_cols)} columnas')\n",
        "\n",
        "# %% Limpieza final\n",
        "X_train = X_train.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
        "X_test = X_test.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
        "\n",
        "print(f'\\n✓ Datos listos')\n",
        "print(f'  NaN en train: {X_train.isnull().sum().sum()}')\n",
        "print(f'  NaN en test: {X_test.isnull().sum().sum()}')\n",
        "\n",
        "# %% Codificar target para XGBoost\n",
        "print('\\n--- Preparando target ---')\n",
        "le_target = LabelEncoder()\n",
        "y_train_encoded = le_target.fit_transform(y_train)\n",
        "\n",
        "print(f'✓ Clases originales: {le_target.classes_}')\n",
        "print(f'✓ Clases codificadas: {np.unique(y_train_encoded)}')\n",
        "\n",
        "# %% Entrenamiento XGBoost\n",
        "print('\\n--- Entrenando XGBoost ---')\n",
        "print('XGBoost es un gradient boosting que construye árboles secuencialmente')\n",
        "\n",
        "xgb_model = XGBClassifier(\n",
        "    n_estimators=200,        # Número de árboles\n",
        "    max_depth=8,             # Profundidad máxima\n",
        "    learning_rate=0.1,       # Tasa de aprendizaje\n",
        "    subsample=0.8,           # Proporción de muestras por árbol\n",
        "    colsample_bytree=0.8,    # Proporción de features por árbol\n",
        "    objective='multi:softmax',  # Clasificación multiclase\n",
        "    num_class=4,             # 4 clases\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    eval_metric='mlogloss'\n",
        ")\n",
        "\n",
        "print('\\nEntrenando modelo...')\n",
        "xgb_model.fit(X_train, y_train_encoded, verbose=False)\n",
        "print('✓ XGBoost entrenado')\n",
        "\n",
        "# %% Evaluación\n",
        "train_score = xgb_model.score(X_train, y_train_encoded)\n",
        "print(f'\\n✓ Accuracy en train: {train_score:.4f}')\n",
        "\n",
        "# %% Validación cruzada\n",
        "print('\\n--- Validación Cruzada ---')\n",
        "cv_scores = cross_val_score(xgb_model, X_train, y_train_encoded, cv=3, n_jobs=-1)\n",
        "print(f'CV Scores: {cv_scores.round(4)}')\n",
        "print(f'✓ CV Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})')\n",
        "\n",
        "# %% Feature Importance\n",
        "print('\\n--- Feature Importance (Top 15) ---')\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': X_train.columns,\n",
        "    'importance': xgb_model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(feature_importance.head(15).to_string(index=False))\n",
        "\n",
        "# %% Predicciones\n",
        "print('\\n--- Predicciones ---')\n",
        "predictions_encoded = xgb_model.predict(X_test)\n",
        "\n",
        "# Decodificar predicciones\n",
        "predictions = le_target.inverse_transform(predictions_encoded)\n",
        "print(f'✓ Predicciones: {len(predictions)}')\n",
        "\n",
        "print('\\nDistribución de predicciones:')\n",
        "pred_counts = pd.Series(predictions).value_counts().sort_index()\n",
        "print(pred_counts)\n",
        "print('\\nPorcentaje:')\n",
        "print((pred_counts / len(predictions) * 100).round(2))\n",
        "\n",
        "# %% Submission\n",
        "submission = pd.DataFrame({\n",
        "    'ID': test_ids,\n",
        "    'RENDIMIENTO_GLOBAL': predictions\n",
        "})\n",
        "\n",
        "print(f'\\n✓ Submission shape: {submission.shape}')\n",
        "print(f'✓ IDs únicos: {submission[\"ID\"].nunique()}')\n",
        "print(f'✓ Clases predichas: {sorted(submission[\"RENDIMIENTO_GLOBAL\"].unique())}')\n",
        "\n",
        "print('\\nPrimeras filas:')\n",
        "print(submission.head(10))\n",
        "\n",
        "# %% Guardar\n",
        "submission.to_csv('submission_xgboost.csv', index=False)\n",
        "print('\\n' + '='*60)\n",
        "print('✓✓✓ Archivo submission_xgboost.csv generado ✓✓✓')\n",
        "print('='*60)\n",
        "print('\\nEste es un modelo alternativo usando XGBoost')\n",
        "\n",
        "# %% [markdown]\n",
        "\"\"\"\n",
        "## Resumen Modelo 04\n",
        "\n",
        "### XGBoost (Gradient Boosting)\n",
        "- **Tipo**: Gradient Boosting Machine\n",
        "- **Árboles**: 200 (secuenciales, no paralelos)\n",
        "- **Learning rate**: 0.1\n",
        "- **Max depth**: 8\n",
        "\n",
        "### Ventajas de XGBoost\n",
        "- ✅ Muy efectivo en competencias Kaggle\n",
        "- ✅ Maneja bien features categóricas codificadas\n",
        "- ✅ Regularización incorporada\n",
        "- ✅ Feature importance integrado\n",
        "\n",
        "### Diferencias con otros modelos\n",
        "\n",
        "| Característica | RF (99) | SVM (03) | XGBoost (04) |\n",
        "|----------------|---------|----------|--------------|\n",
        "| Construcción | Paralela | N/A | Secuencial |\n",
        "| Velocidad | Rápido | Lento | Media |\n",
        "| Overfitting | Bajo | Medio | Controlable |\n",
        "| Interpretabilidad | Alta | Baja | Alta |\n",
        "\n",
        "### Conclusión\n",
        "XGBoost es una excelente alternativa que frecuentemente da buenos\n",
        "resultados en problemas de clasificación tabulares.\n",
        "\"\"\""
      ]
    }
  ]
}